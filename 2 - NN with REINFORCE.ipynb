{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla NN with REINFORCE\n",
    "\n",
    "Implementing a one hidden layer vanilla Neural Network that uses REINFORCE as the policy gradient method. The implementation will be on PyTorch instead of Tensorflow as people seem to believe it is a better framework. (I don't have any prior PyTorch experience so I am learning that on the go as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the networkâ€™s parameters\n",
    "- Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n",
    "\n",
    "###### from [pytorch-tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "###### [other sources](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "import torch.distributions as TDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function version\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_nodes = [4,128,2], add_dropouts = True, dropout_rate = 0.6):\n",
    "        super(Policy, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.add_dropouts = add_dropouts\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()        \n",
    "        \n",
    "        self.layers.append(nn.Linear(self.num_nodes[0], self.num_nodes[1]))\n",
    "        self.dropouts.append(nn.Dropout(p = dropout_rate))\n",
    "        \n",
    "        for i in range(1, len(num_nodes)-2):\n",
    "            self.layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
    "            self.dropouts.append(nn.Dropout(p = dropout_rate))\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.num_nodes[-2], self.num_nodes[-1]))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for layer, dropout in zip(self.layers[:-1], self.dropouts):\n",
    "            if self.add_dropouts:\n",
    "                X = F.relu(dropout(layer(X)))\n",
    "            else:\n",
    "                X = F.relu(layer(X))\n",
    "  \n",
    "        return F.softmax(self.layers[-1](X), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HexBoard:\n",
    "    def __init__(self, BOARD_SIZE=[3, 3]):\n",
    "        self.BOARD_SIZE = BOARD_SIZE\n",
    "        self.BOARD = [['.' for __ in range(self.BOARD_SIZE[0])] for _ in range(self.BOARD_SIZE[1])]\n",
    "        # self.BOARD = [\n",
    "        #     ['B','W','W'],\n",
    "        #     ['W','W','B'],\n",
    "        #     ['B','W','B'],\n",
    "        # ]\n",
    "        self.done = False # game is over or not\n",
    "\n",
    "    # RUNNER FUNCTIONS ____________\n",
    "    # _____________________________\n",
    "    # _____________________________\n",
    "    # _____________________________\n",
    "\n",
    "    def step(self, color, action):\n",
    "        # color = 'B' or 'W'\n",
    "        # action = [x, y]\n",
    "        try:\n",
    "            input_err = self.placeStone(action, color) # False if there is an error in the input\n",
    "            result = self.check_game_status()\n",
    "        except Exception:\n",
    "            return 0, 0, 0, 0, False\n",
    "        # reward system: win +1 / loss -1\n",
    "        if result == color:\n",
    "            reward = 1\n",
    "        elif result == '=':\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "        return self.BOARD, self.done, result, reward, input_err\n",
    "\n",
    "    # HELPER FUNCTIONS ____________\n",
    "    # _____________________________\n",
    "    # _____________________________\n",
    "    # _____________________________\n",
    "\n",
    "    def checkEdge(self, color, node):\n",
    "        if color == 'W' and node[1] == self.BOARD_SIZE[1]-1:\n",
    "            return True\n",
    "        if color == 'B' and node[0] == self.BOARD_SIZE[0]-1:\n",
    "            return True\n",
    "        return False\n",
    "                \n",
    "    def testConnections(self, cellToCheck):\n",
    "        print('connections are', self.cell_connections(cellToCheck))\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(self.BOARD_SIZE[0]):\n",
    "            print('  '*(self.BOARD_SIZE[0]-i-1), end='')\n",
    "            for j in range(self.BOARD_SIZE[1]):\n",
    "                print(self.BOARD[i][j], end=' ')\n",
    "            print('')\n",
    "\n",
    "    def placeStone(self, cell, color):\n",
    "        if self.BOARD[cell[0]][cell[1]] != '.':\n",
    "            print('Invalid Action')\n",
    "            return False\n",
    "        self.BOARD[cell[0]][cell[1]] = color\n",
    "        return True\n",
    "\n",
    "    def cell_connections(self, cell):\n",
    "        row = cell[0] \n",
    "        col = cell[1]\n",
    "\n",
    "        positions = []\n",
    "        \n",
    "        if col + 1 < self.BOARD_SIZE[1]:\n",
    "            positions.append([row, col + 1])\n",
    "        if col - 1 >= 0:\n",
    "            positions.append([row, col - 1])\n",
    "        if row + 1 < self.BOARD_SIZE[0]:\n",
    "            positions.append([row + 1, col])\n",
    "            if col + 1 < self.BOARD_SIZE[1]:\n",
    "                positions.append([row + 1, col + 1])\n",
    "        if row - 1 >= 0:\n",
    "            positions.append([row - 1, col])\n",
    "            if col - 1 >= 0:\n",
    "                positions.append([row - 1, col - 1])\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def check_game_status(self):\n",
    "        # checking for white\n",
    "        self.CHECK_BOARD = [[False for __ in range(self.BOARD_SIZE[0])] for _ in range(self.BOARD_SIZE[1])] \n",
    "        for i in range(self.BOARD_SIZE[0]):\n",
    "            if self.BOARD[i][0] == 'W':\n",
    "                self.CHECK_BOARD[i][0] = True\n",
    "                self.check_connections(self.cell_connections([i, 0]), 'W')\n",
    "                if self.done:\n",
    "                    return 'W'\n",
    "        # checking for black\n",
    "        self.CHECK_BOARD = [[False for __ in range(self.BOARD_SIZE[0])] for _ in range(self.BOARD_SIZE[1])] \n",
    "        for i in range(self.BOARD_SIZE[1]):\n",
    "            if self.BOARD[0][i] == 'B':\n",
    "                self.CHECK_BOARD[0][i] = True\n",
    "                self.check_connections(self.cell_connections([0, i]), 'B')\n",
    "                if self.done:\n",
    "                    return 'B'\n",
    "        return '='\n",
    "\n",
    "    def check_connections(self, connections, color):\n",
    "        for c in connections:\n",
    "            if self.BOARD[c[0]][c[1]] == color and not self.CHECK_BOARD[c[0]][c[1]]:\n",
    "                # print(c[0], c[1], 'visited')\n",
    "                if self.checkEdge(color, c):\n",
    "                    self.done = True\n",
    "                    return\n",
    "                self.CHECK_BOARD[c[0]][c[1]] = True\n",
    "                self.check_connections(self.cell_connections([c[0], c[1]]), color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    \n",
    "    # Discounted reward\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    # Normalized discounted rewards\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    # Policy losses saved -Log * R - Reinforce Formula\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    \n",
    "    # reset the grad log\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # concatinate the policy losees and sum\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    \n",
    "    # gradients taken\n",
    "    policy_loss.backward()\n",
    "    \n",
    "    # Policy Updated\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Logs deleted for next try\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy):\n",
    "    # Flattening\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0) \n",
    "    \n",
    "    # probabilities given policy\n",
    "    probs = policy(state) \n",
    "\n",
    "    # get the distribution according to the probs of each value\n",
    "    m = TDist.Categorical(probs) \n",
    "    # get a sample by given probs\n",
    "    action = m.sample() \n",
    "\n",
    "    # save the log_prob of the action\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    # Why are we keeping the log prob ?\n",
    "    \n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: -75.67\tAverage reward: 5.72\n",
      "Episode 100\tLast reward: -207.12\tAverage reward: -141.66\n",
      "Episode 200\tLast reward: -298.06\tAverage reward: -119.06\n",
      "Episode 300\tLast reward: -1.40\tAverage reward: -101.15\n",
      "Episode 400\tLast reward: 21.95\tAverage reward: -25.13\n",
      "Episode 500\tLast reward: -93.21\tAverage reward: -54.36\n",
      "Episode 600\tLast reward: 82.66\tAverage reward: 1.94\n",
      "Episode 700\tLast reward: -196.67\tAverage reward: -80.34\n",
      "Episode 800\tLast reward: 69.28\tAverage reward: 62.57\n",
      "Episode 900\tLast reward: 211.06\tAverage reward: 40.56\n",
      "Episode 1000\tLast reward: -28.61\tAverage reward: 24.50\n",
      "Episode 1100\tLast reward: -85.98\tAverage reward: -16.44\n",
      "Episode 1200\tLast reward: -50.47\tAverage reward: 28.13\n",
      "Episode 1300\tLast reward: 1.07\tAverage reward: 39.17\n",
      "Episode 1400\tLast reward: -42.39\tAverage reward: 97.06\n",
      "Episode 1500\tLast reward: 9.21\tAverage reward: 69.35\n",
      "Episode 1600\tLast reward: -23.79\tAverage reward: -13.47\n",
      "Episode 1700\tLast reward: 101.74\tAverage reward: 48.42\n",
      "Episode 1800\tLast reward: 166.48\tAverage reward: 104.65\n",
      "Episode 1900\tLast reward: 68.90\tAverage reward: 47.88\n",
      "Episode 2000\tLast reward: 27.89\tAverage reward: 41.76\n",
      "Episode 2100\tLast reward: 238.10\tAverage reward: 96.12\n",
      "Episode 2200\tLast reward: 120.49\tAverage reward: 107.26\n",
      "Episode 2300\tLast reward: -24.51\tAverage reward: 56.13\n",
      "Episode 2400\tLast reward: -21.55\tAverage reward: 50.45\n",
      "Episode 2500\tLast reward: -25.98\tAverage reward: -7.48\n",
      "Episode 2600\tLast reward: -30.15\tAverage reward: -7.53\n",
      "Episode 2700\tLast reward: -25.39\tAverage reward: 1.15\n",
      "Episode 2800\tLast reward: -32.99\tAverage reward: 35.73\n",
      "Episode 2900\tLast reward: -113.49\tAverage reward: -72.71\n",
      "Episode 3000\tLast reward: -76.32\tAverage reward: -69.60\n",
      "Episode 3100\tLast reward: 113.70\tAverage reward: 74.43\n",
      "Episode 3200\tLast reward: 210.24\tAverage reward: 52.00\n",
      "Episode 3300\tLast reward: -88.02\tAverage reward: -48.41\n",
      "Episode 3400\tLast reward: -23.33\tAverage reward: 43.52\n",
      "Episode 3500\tLast reward: 34.68\tAverage reward: 16.84\n",
      "Episode 3600\tLast reward: 38.27\tAverage reward: 68.73\n",
      "Episode 3700\tLast reward: -14.54\tAverage reward: 91.80\n",
      "Episode 3800\tLast reward: 73.07\tAverage reward: 97.64\n",
      "Episode 3900\tLast reward: 151.02\tAverage reward: 95.57\n",
      "Episode 4000\tLast reward: -22.68\tAverage reward: 61.65\n",
      "Episode 4100\tLast reward: -143.45\tAverage reward: 38.61\n",
      "Episode 4200\tLast reward: -21.57\tAverage reward: 21.86\n",
      "Episode 4300\tLast reward: 214.03\tAverage reward: 86.08\n",
      "Episode 4400\tLast reward: -78.35\tAverage reward: 13.19\n",
      "Episode 4500\tLast reward: 82.54\tAverage reward: 62.60\n",
      "Episode 4600\tLast reward: 184.51\tAverage reward: 29.04\n",
      "Episode 4700\tLast reward: -39.19\tAverage reward: 39.46\n",
      "Episode 4800\tLast reward: 53.82\tAverage reward: 53.03\n",
      "Episode 4900\tLast reward: 71.23\tAverage reward: 47.25\n",
      "Episode 5000\tLast reward: -69.84\tAverage reward: 13.04\n",
      "Episode 5100\tLast reward: 19.45\tAverage reward: 31.39\n",
      "Episode 5200\tLast reward: 72.83\tAverage reward: 81.90\n",
      "Episode 5300\tLast reward: 217.57\tAverage reward: 101.22\n",
      "Episode 5400\tLast reward: 40.24\tAverage reward: 76.48\n",
      "Episode 5500\tLast reward: 271.28\tAverage reward: 58.88\n",
      "Episode 5600\tLast reward: 33.86\tAverage reward: 50.35\n",
      "Episode 5700\tLast reward: 76.14\tAverage reward: 81.02\n",
      "Episode 5800\tLast reward: 53.81\tAverage reward: 58.73\n",
      "Episode 5900\tLast reward: 69.51\tAverage reward: 45.77\n",
      "Episode 6000\tLast reward: 223.76\tAverage reward: 78.99\n",
      "Episode 6100\tLast reward: 48.13\tAverage reward: 92.83\n",
      "Episode 6200\tLast reward: -1.33\tAverage reward: 101.55\n",
      "Episode 6300\tLast reward: 195.86\tAverage reward: 90.48\n",
      "Episode 6400\tLast reward: 227.75\tAverage reward: 89.34\n",
      "Episode 6500\tLast reward: 212.81\tAverage reward: 107.65\n",
      "Episode 6600\tLast reward: 248.09\tAverage reward: 131.82\n",
      "Episode 6700\tLast reward: 45.47\tAverage reward: 97.43\n",
      "Episode 6800\tLast reward: -12.15\tAverage reward: 99.56\n",
      "Episode 6900\tLast reward: 70.81\tAverage reward: 75.23\n",
      "Episode 7000\tLast reward: -16.35\tAverage reward: 92.57\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     env = gym.make('LunarLander-v2')\n",
    "    env = HexBoard()\n",
    "\n",
    "#     filepath = 'model/'\n",
    "#     if not os.path.exists(filepath):\n",
    "#         os.makedirs(filepath)\n",
    "    \n",
    "    num_of_episodes = 100\n",
    "    num_of_steps = 10\n",
    "\n",
    "    running_reward = 1\n",
    "\n",
    "    log_interval = 100\n",
    "    gamma = 0.99\n",
    "\n",
    "#     num_of_nodes = [env.observation_space.shape[0], 128,env.action_space.n]\n",
    "    num_of_nodes = [env.]\n",
    "\n",
    "    policy = Policy(num_of_nodes, add_dropouts=True)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    for ep in range(num_of_episodes):\n",
    "\n",
    "        obs = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        for step in range(num_of_steps):\n",
    "            action = select_action(obs, policy)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "    #         if ep % log_interval == 0:\n",
    "    #             env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if ep % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  ep, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, step))\n",
    "            break\n",
    "\n",
    "        ######## SAVING THE MODEL TO CONTINUE TRAINING LATER ###########\n",
    "        state = {\n",
    "            'episode': ep,\n",
    "            'state_dict': policy.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, filepath + 'policy_state.pth')\n",
    "        #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
